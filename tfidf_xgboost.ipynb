{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementatin of tfidf with xg boost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "'''\n",
    "To import any module, run following command in ipynb\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "'''\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n",
    "from ipynb.fs.full.Data_preprocessing import clean\n",
    "\n",
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep=\"\\t\")\n",
    "\n",
    "data['question1'] = data['question1'].apply(clean)\n",
    "data['question2'] = data['question2'].apply(clean)\n",
    "\n",
    "#data.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor (koh-i-noor) dia...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely? how can i solve...</td>\n",
       "      <td>find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4  what is the story of kohinoor (koh-i-noor) dia...   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8  why am i mentally very lonely? how can i solve...   \n",
       "4   4     9    10  which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning of the data\n",
    "\n",
    "# from ipynb.fs.full.Data_preprocessing import clean\n",
    "\n",
    "#data.isnull().values.ravel().sum()\n",
    "\n",
    "data = pd.read_csv('CleanData.tsv',sep='\\t')\n",
    "#data1.isnull().values.ravel().sum()\n",
    "data.describe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram tfidf \n",
    "\n",
    "'''\n",
    "Term Frequency (tf): gives us the frequency of the word in each document in the corpus. \n",
    "It is the ratio of number of times the word appears in a document compared to the total \n",
    "number of words in that document. It increases as the number of occurrences of that word \n",
    "within the document increases. Each document has its own tf.\n",
    "\n",
    "Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents \n",
    "in the corpus. The words that occur rarely in the corpus have a high IDF score.\n",
    "\n",
    "Combining these two we come up with the TF-IDF score for a word in a document in the corpus.\n",
    "It is the product of tf and idf:\n",
    "\n",
    "\n",
    "'''\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit( pd.Series(list(train_data['question1']) + list(train_data['question2'])).astype(str).unique() )\n",
    "tfidf_train_1 = tfidf_vect.transform(train_data['question1'])\n",
    "tfidf_train_2 = tfidf_vect.transform(train_data['question2'])\n",
    "X_train = scipy.sparse.hstack((tfidf_train_1, tfidf_train_2 ))\n",
    "label1 = train_data['is_duplicate'].values\n",
    "\n",
    "\n",
    "\n",
    "tfidf_test_1 = tfidf_vect.transform(test_data['question1'])\n",
    "tfidf_test_2 = tfidf_vect.transform(test_data['question2'])\n",
    "X_test = scipy.sparse.hstack((tfidf_test_1, tfidf_test_2 ))\n",
    "label2 = test_data['is_duplicate'].values\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, label1) \n",
    "xgb_prediction = xgb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(xgb_prediction, label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(xgb_prediction , label2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(xgb_prediction , label2, average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(xgb_prediction , label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Is there anyone here who didn't work too hard and still got into an IIT?\"\n",
    "q2 = \"Is there anyone here who didn't work too hard and still got into an IIT?\"\n",
    "\n",
    "q1 = clean(q1)\n",
    "q2 = clean(q2)\n",
    "\n",
    "print(q1)\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tfidf_vect.transform([q1])\n",
    "v2 = tfidf_vect.transform([q2])\n",
    "\n",
    "X1= scipy.sparse.hstack((v1, v2 ))\n",
    "xgb_model.predict(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "ntfidf_vect.fit( pd.Series(list(train_data['question1']) + list(train_data['question2'])).astype(str).unique() )\n",
    "ntfidf_train_1 = ntfidf_vect.transform(train_data['question1'])\n",
    "ntfidf_train_2 = ntfidf_vect.transform(train_data['question2'])\n",
    "nX_train = scipy.sparse.hstack((ntfidf_train_1, ntfidf_train_2 ))\n",
    "nlabel1 = train_data['is_duplicate'].values\n",
    "\n",
    "\n",
    "\n",
    "ntfidf_test_1 = ntfidf_vect.transform(test_data['question1'])\n",
    "ntfidf_test_2 = ntfidf_vect.transform(test_data['question2'])\n",
    "nX_test = scipy.sparse.hstack((ntfidf_test_1, ntfidf_test_2 ))\n",
    "nlabel2 = test_data['is_duplicate'].values\n",
    "\n",
    "\n",
    "nxgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(nX_train, nlabel1) \n",
    "nxgb_prediction = nxgb_model.predict(nX_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7362206996627834\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(nxgb_prediction, nlabel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70051 25684]\n",
      " [ 6309 19243]]\n"
     ]
    }
   ],
   "source": [
    "ncm = confusion_matrix(nxgb_prediction , nlabel2)\n",
    "print(ncm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800801157482013"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf1 = f1_score(nxgb_prediction , nlabel2, average='macro')\n",
    "nf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81     95735\n",
      "           1       0.43      0.75      0.55     25552\n",
      "\n",
      "   micro avg       0.74      0.74      0.74    121287\n",
      "   macro avg       0.67      0.74      0.68    121287\n",
      "weighted avg       0.81      0.74      0.76    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(nxgb_prediction , nlabel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pickle\\nf = open('tfidf_xgb.pickle', 'wb')\\npickle.dump(xgb_model, f)\\nf.close()\\n\\nf = open('ntfidf_xgb.pickle', 'wb')\\npickle.dump(nxgb_model, f)\\nf.close()\\n\\nimport pickle\\nf = open('tfidf_vec_xgb.pickle', 'wb')\\npickle.dump(tfidf_vect, f)\\nf.close()\\n\\nf = open('ntfidf_vec_xgb.pickle', 'wb')\\npickle.dump(ntfidf_vect, f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "f = open('tfidf_xgb.pickle', 'wb')\n",
    "pickle.dump(xgb_model, f)\n",
    "f.close()\n",
    "\n",
    "f = open('ntfidf_xgb.pickle', 'wb')\n",
    "pickle.dump(nxgb_model, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "f = open('tfidf_vec_xgb.pickle', 'wb')\n",
    "pickle.dump(tfidf_vect, f)\n",
    "f.close()\n",
    "\n",
    "f = open('ntfidf_vec_xgb.pickle', 'wb')\n",
    "pickle.dump(ntfidf_vect, f)\n",
    "f.close()\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
