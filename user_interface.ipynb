{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo import any module x\\nimport sys\\n!{sys.executable} -m pip install x\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "from tkinter import font\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipynb.fs.full.Data_preprocessing import clean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "from ipynb.fs.full.Data_preprocessing import clean\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "'''\n",
    "To import any module x\n",
    "import sys\n",
    "!{sys.executable} -m pip install x\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AnsData.csv', header = None, sep='\\t' )\n",
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ipynb.fs.full.Data_preprocessing import clean\n",
    "\n",
    "data['question1'] = data['question1'].apply(clean)\n",
    "data['question2'] = data['question2'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.init_sims(replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(max_features = 50000)\n",
    "tfidf_vec.fit(pd.Series(data['question1']+data['question2']) .astype(str).unique() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ntfidf_vect = TfidfVectorizer(max_features = 50000, ngram_range=(2,2))\n",
    "ntfidf_vect.fit(pd.Series(data['question1']+data['question2']) .astype(str).unique() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(q1, q2, ans1, ans2):\n",
    "    \n",
    "    info=\"\"\n",
    "    q1 = clean(q1)\n",
    "    q2 = clean(q2)\n",
    "    ans1 = clean(ans1)\n",
    "    ans2 = clean(ans2)\n",
    "    \n",
    "    da = distance(ans1, ans2)\n",
    "    dq = distance(q1, q2)\n",
    "\n",
    "    \n",
    "    t1 = tfidf_vec.transform([ans1])\n",
    "    t2 = tfidf_vec.transform([ans2])\n",
    "    tda = cosine_similarity(t1, t2)\n",
    "\n",
    "    \n",
    "    t1 = tfidf_vec.transform([q1])\n",
    "    t2 = tfidf_vec.transform([q2])\n",
    "    tdq = cosine_similarity(t1, t2)\n",
    "\n",
    "    X = np.zeros((1,4))\n",
    "    \n",
    "    X[0][0] = dq\n",
    "    X[0][1] = da\n",
    "    X[0][2] = tdq\n",
    "    X[0][3] = tda\n",
    "\n",
    "    print(X)\n",
    "    \n",
    "    print(logreg.predict(X))\n",
    "    print(random_forest.predict(X))\n",
    "    print(linear_svc.predict(X))\n",
    "    print(sgd.predict(X))\n",
    "    print(gaussian.predict(X))\n",
    "    print(knn.predict(X))\n",
    "    print(votingC.predict(X))\n",
    "    \n",
    "    info+= \"Is Duplicate:\\n\"\n",
    "    info+=\"Using Logistic Regression: {}\\n\".format(logreg .predict(X))\n",
    "    info+=\"Using Random Forest: {}\\n\".format(random_forest.predict(X))\n",
    "    info+=\"Using Linear SVC: {}\\n\".format( linear_svc.predict(X))\n",
    "    info+=\"Using SGD: {}\\n\".format( sgd.predict(X))\n",
    "    info+=\"Using Guassian NB: {}\\n\".format(gaussian.predict(X))\n",
    "    info+=\"Using KNN: {}\\n\".format( knn.predict(X))\n",
    "    info+=\"Using VotingC: {}\\n\".format(votingC.predict(X))\n",
    "    \n",
    "    return info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(tfidf_1, tfidf_2):\n",
    "    return cosine(tfidf_1.todense(),tfidf_2.todense())\n",
    "\n",
    "\n",
    "def wmd(s1,s2):\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "def distance(s1, s2):\n",
    "    \n",
    "    \n",
    "    num = len(s1)\n",
    "          \n",
    "    sent1 = [word for word in s1.split() if word in model.vocab]\n",
    "    sent2 = [word for word in s2.split() if word in model.vocab]\n",
    "        \n",
    "    if len(sent1)>0 and len(sent2)>0:\n",
    "        return model.wmdistance(sent1,sent2)\n",
    "    else:\n",
    "        return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.read_csv('wmd_distance_ans.csv', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2017/rahul.cs17/anaconda3/envs/rahul/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/stud/btech/cse/2017/rahul.cs17/anaconda3/envs/rahul/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/home/stud/btech/cse/2017/rahul.cs17/anaconda3/envs/rahul/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/stud/btech/cse/2017/rahul.cs17/anaconda3/envs/rahul/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=5)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "votingC = VotingClassifier(estimators=[('lg', logreg), ('rf', random_forest),\n",
    " ('svc' , linear_svc),('sgd',sgd),('gbc',gaussian), ('knn', knn)])\n",
    "votingC = votingC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = train_test_split(dist, test_size = 0.1, random_state = 0)\n",
    "X_train, Y_train = train_data.loc[:, :3], train_data.loc[:, 4]\n",
    "X_test, Y_test = test_data.loc[:, :3], test_data.loc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16682849 1.31598484 0.3328098  1.        ]]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "Is Duplicate:\n",
      "Using Logistic Regression: [0.]Using Random Forest: [0.]Using Linear SVC: [0.]Using SGD: [0.]Using Guassian NB: [0.]Using KNN: [1.]Using VotingC: [0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root = Tk()\n",
    "root.title('Find similarity between two questions')\n",
    "\n",
    "\n",
    "def callback():\n",
    "    \n",
    "    # Get both questions\n",
    "    q1= que1.get(1.0, END)\n",
    "    a1= ans1.get(1.0, END)\n",
    "    q2= que2.get(1.0, END)\n",
    "    a2= ans2.get(1.0, END)\n",
    "    \n",
    "    # Open new window\n",
    "    new_window = Toplevel(root)\n",
    "    new_window.title('Similarity between two questions')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Printing question 1\n",
    "    text1 = Message(new_window, text= 'Question1:\\n\\n' + q1, anchor = 'w',  aspect = 1000)\n",
    "    text1.pack(fill = X)\n",
    "    \n",
    "    # Printing question 2\n",
    "    text2 = Message(new_window, text= '\\n\\nAnswer1:\\n\\n' + a1 , anchor = 'w',  aspect = 1000)\n",
    "    text2.pack(fill = X)\n",
    "    \n",
    "    # Printing ans 1\n",
    "    text1 = Message(new_window, text= 'Question2:\\n\\n' + q2, anchor = 'w',  aspect = 1000)\n",
    "    text1.pack(fill = X)\n",
    "    \n",
    "    # Printing ans 2\n",
    "    text2 = Message(new_window, text= '\\n\\nAnswer2:\\n\\n' + a2 , anchor = 'w',  aspect = 1000)\n",
    "    text2.pack(fill = X)\n",
    "    \n",
    "    # Cleaning both questions\n",
    "    q1 = clean(q1)\n",
    "    q2 = clean(q2)\n",
    "    a1 = clean(a1)\n",
    "    a2 = clean(a2)\n",
    "    \n",
    "    \n",
    "    # Using word2vec model\n",
    "    sent1 = [word for word in q1.split() if word in model.vocab]\n",
    "    sent2 = [word for word in q2.split() if word in model.vocab]\n",
    "    \n",
    "    l1 = Label( new_window, text= '\\n\\nUsing word2vec:\\n\\n', anchor = 'w' )\n",
    "    l1.pack(fill = X)\n",
    "    text3 = Message(new_window, text='Distance:\\n\\n'+ str(model.wmdistance(sent1, sent2)), anchor = 'w', aspect = 500)\n",
    "    text3.pack(fill = X)\n",
    "    \n",
    "    \n",
    "    # Using unigram tfidf with cosine similarity\n",
    "    uni_dist = str( cosine_similarity( tfidf_vec.transform([q1]) , tfidf_vec.transform([q2]) ) )\n",
    "   \n",
    "    \n",
    "    l2 = Label( new_window, text='\\n\\nUsing unigram:\\n\\n',  anchor = 'w')\n",
    "    l2.pack(fill = X)  \n",
    "    text4 = Message(new_window, text='Distance\\n\\n: '+ uni_dist, anchor = 'w',  aspect = 1000)\n",
    "    text4.pack(fill = X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Using ngram tfidf with cosine similarity\n",
    "    n_dist = str( cosine_similarity( ntfidf_vect.transform([q1]) , ntfidf_vect.transform([q2]) ) )\n",
    "        \n",
    "    l3 = Label( new_window, text='\\n\\nUsing n-gram:\\n\\n',  anchor = 'w')\n",
    "    l3.pack(fill = X)   \n",
    "    text5 = Message(new_window, text='Distance\\n\\n: '+ n_dist, anchor = 'w', aspect = 1000)\n",
    "    text5.pack(fill = X)\n",
    "    \n",
    "    l4 = Label( new_window, text='\\n\\nUsing answers:\\n\\n',  anchor = 'w')\n",
    "    l4.pack(fill = X)   \n",
    "    \n",
    "    ans =\"\"\n",
    "    if len(a1)==0 or len(a2)==0:\n",
    "        ans =\"Answer NOT FOUND\"\n",
    "    else:\n",
    "        ans = predict(q1, q2, a1, a2)\n",
    "    print(ans)    \n",
    "    text5 = Message(new_window, text=ans, anchor = 'w',  aspect = 1000)\n",
    "    text5.pack(fill = X)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Fonts used \n",
    "Font1 = font.Font(family=\"Comic Sans MS\", size=10)\n",
    "Font2 = font.Font(family=\"Arial\", size=9)\n",
    "   \n",
    "\n",
    "# top Frame to contain 1st question\n",
    "topFrameq = Frame(root, padx = 10, pady=5)\n",
    "topFrameq.pack()\n",
    "\n",
    "# top Frame to contain 1st ans\n",
    "topFramea = Frame(root, padx = 10, pady=5)\n",
    "topFramea.pack()\n",
    "\n",
    "# Mid Frame to contain 2nd question\n",
    "MidFrameq = Frame(root, padx=10, pady=5)\n",
    "MidFrameq.pack()\n",
    "\n",
    "# Mid Frame to contain 2nd question\n",
    "MidFramea = Frame(root, padx=10, pady=5)\n",
    "MidFramea.pack()\n",
    "\n",
    "# Bottom frame to contain button\n",
    "bottomFrame = Frame(root, padx = 5, pady=5)\n",
    "bottomFrame.pack(side= BOTTOM)\n",
    "\n",
    "# Top Frame\n",
    "label1 = Label( topFrameq, text='Enter Question1:', font=Font1)\n",
    "label1.pack(side=LEFT)\n",
    "que1 = Text(topFrameq, padx = 10, pady =10, font=Font2, height=7)\n",
    "que1.focus_set()\n",
    "que1.pack(side = LEFT)\n",
    "\n",
    "label3 = Label( topFramea, text='Enter Answer 1:', font=Font1)\n",
    "label3.pack(side=LEFT)\n",
    "ans1 = Text(topFramea, padx = 10, pady =10, font=Font2, height=13)\n",
    "ans1.focus_set()\n",
    "ans1.pack(side = LEFT)\n",
    "\n",
    "# Mid Frame\n",
    "label2 = Label(MidFrameq , text='Enter Question2:', font=Font1)\n",
    "label2.pack(side=LEFT)\n",
    "que2 = Text(MidFrameq,  padx = 10, pady =10, font=Font2, height=7)\n",
    "que2.focus_set()\n",
    "que2.pack(side = LEFT)\n",
    "\n",
    "label3 = Label(MidFramea , text='Enter Answer 2:', font=Font1)\n",
    "label3.pack(side=LEFT)\n",
    "ans2 = Text(MidFramea, padx = 15, pady =10, font=Font2, height=13)\n",
    "ans2.focus_set()\n",
    "ans2.pack(side = LEFT)\n",
    " \n",
    "# Bottom Frame\n",
    "b = Button(bottomFrame, text=\"Find similarity\", width=20, fg='black', activebackground='green' ,command=callback)\n",
    "b.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Word2vec model\n",
    "# f = open('Google_word2vec.pickle', 'rb')\n",
    "# model = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# # ntfidf with cosine similarity\n",
    "# f = open('ntfidf_vec.pickle', 'rb')\n",
    "# ntfidf_vec = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# # tfidf with cosine similarity\n",
    "# f = open('tfidf_vec.pickle', 'rb')\n",
    "# tfidf_vec = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# # tfidf with xgboost\n",
    "\n",
    "# f = open('tfidf_vec_xgb.pickle', 'rb')\n",
    "# tfidf_vec_xgb = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('tfidf_xgb.pickle', 'rb')\n",
    "# tfidf_xgb = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# # ntfidf with xgboost\n",
    "\n",
    "# f = open('ntfidf_vec_xgb.pickle', 'rb')\n",
    "# ntfidf_vec_xgb = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# f = open('ntfidf_xgb.pickle', 'rb')\n",
    "# ntfidf_xgb = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# # f = open('bst_xgb.pickle', 'rb')\n",
    "# # bst = pickle.load(f)\n",
    "# # f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
